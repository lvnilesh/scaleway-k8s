---
- name: kubernetes multimasters
  block:

  - name: modprobe
    modprobe:
      name: "{{ item }}"
      state: present
    with_items:
      - ip_vs
      - nf_conntrack_ipv4

  - name: Fixing nasty bug
    shell: "iptables -P FORWARD ACCEPT"
    ignore_errors: True

  # ssh key sharing between first and other masters
  - name: Creates an ssh key on first master
    shell: ssh-keygen -b 2048 -t rsa -f /root/.ssh/id_rsa -q -N ""
    args:
      creates: /root/.ssh/id_rsa
    when:
      - inventory_hostname == groups['masters'][0]

  - name: Register pub key on first master
    shell: cat /root/.ssh/id_rsa.pub
    register: first_master_key
    when:
      - inventory_hostname == groups['masters'][0]
  
  - name: Propagate the key to other masters
    authorized_key:
      key: "{{ hostvars[groups['masters'][0]]['first_master_key'].stdout }}"
      user: root
      state: present
    when:
      - "'masters' in group_names"
      - inventory_hostname != groups['masters'][0]

  - name: Creating /etc/keepalived on master nodes
    file:
      path: /etc/keepalived
      state: directory
    when: "'master in group_names'"

  - name: Templating /etc/keepalived/keepalived.conf
    template:
      src: keepalived.conf.j2
      dest: /etc/keepalived/keepalived.conf
    when: "'masters' in group_names"

  - name: Running keepalived container on masters nodes
    docker_container:
      name: keepalived_api
      image: "chmod666/keepalived:latest"
      state: started
      detach: True
      volumes:
        - /etc/keepalived/keepalived.conf:/usr/local/etc/keepalived/keepalived.conf
      capabilities:
        - NET_ADMIN
      network_mode: host
      restart_policy: always
    when: "'masters' in group_names"

  - name: Wait for keepalived to be alive :-)
    shell: 'docker ps | grep chmod666/keepalived | grep "Up" | grep "minutes ago"'
    register: result
    until: result.stdout.find("chmod666/keepalived") != -1
    retries: 18
    delay: 10
    # keepalived runs on masters nodes
    when:
      - "'masters' in group_names"

  # at this point everyone should be able to ping the api floating ip
  # add a test here and don't continue until everyone does not ping the api ip

  - name: Adding Kubernetes official gpg key (without proxy)
    apt_key:
      url: "{{ kubernetes_apt_key }}"
      state: present
    when:
      # the only node to have an internet access is the first proxy node
      - "'proxy' in group_names"
      - inventory_hostname == groups['proxy'][0]

  - name: Adding Kubernetes official gpg key (with proxy)
    apt_key:
      url: "{{ kubernetes_apt_key }}"
      state: present
    environment:
      https_proxy: "http://{{ hostvars[groups['proxy'][0]]['ansible_tun0']['ipv4']['address'] }}:8888"
    when:
      # all the other node do not have an internet access and need to use the proxy
      - inventory_hostname != groups['proxy'][0]

  - name: Setting Kubernetes repository depending Ubuntu version
    set_fact:
      kubernetes_repository: "deb http://apt.kubernetes.io/ kubernetes-{{ kubernetes_release }} {{ kubernetes_apt_channel }}"

  - name: Checking Kubernetes repostiory
    debug: var=kubernetes_repository

  - name: Adding Kubernetes repository
    apt_repository:
      repo: "{{ kubernetes_repository }}"
      state: present
      filename: 'kubernetes'

  - name: Installing kubernetes core components (kubectl, kubelet ...)
    apt:
      name: "{{ item }}"
      state: latest
    with_items:
      - kubelet
      - kubeadm
      - kubectl
      - kubernetes-cni

  # generate a new token for the first masters
  - name: Generating a kubeadm token
    shell: "kubeadm token generate"
    register: kubeadm_token
    when:
      - inventory_hostname == groups["masters"][0]

  # generating the kubeadm config file only on master nodes
  - name: Creating kubeadm_config file
    template:
      src: kubeadm-config.j2
      dest: /tmp/kubeadm_config
    when:
      - "'masters' in group_names"

  # we get kubectl output to check if we have to add the node or not
  # errors are ignore in case of the first initialization on the first master
  - name: Getting kubectl outpout
    shell: "kubectl get nodes"
    ignore_errors: True
    when: inventory_hostname == groups["masters"][0]
    register: kubectl_output

  - name: Kubeadm init on first master
    shell: "kubeadm init --config /tmp/kubeadm_config"
    register: kubeadm_output
    when:
      # only run this command if this is the fist master
      # and this one hasn't be initialized
      - inventory_hostname == groups["masters"][0]
      - inventory_hostname not in kubectl_output.stdout

  - name: Kubedadm output
    debug: var=kubeadm_output
    when: inventory_hostname == groups["masters"][0]

  - name: Creating .kube file in $HOME
    # this will be needed on all masters nodes
    file:
      path: ~/.kube
      state: directory
    when:
      - "'masters' in group_names"

  - name: Copying /etc/kubernetes/admin.conf to ~/.kube/config
    copy:
      src: /etc/kubernetes/admin.conf
      dest: ~/.kube/config
      remote_src: yes
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Running first kubectl
    shell: "kubectl get nodes"
    register: first_kubectl
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking kubectl
    debug: var=first_kubectl
    when:
      - inventory_hostname == groups["masters"][0]

  # flannel deployement
  - name: Getting calico manifest
    get_url:
      url: "{{ flannel_manifest }}"
      dest: /tmp/flannel.yml
    environment:
      # calico manifest is downloaded via http
      http_proxy: "http://{{ hostvars[groups['proxy'][0]]['ansible_tun0']['ipv4']['address'] }}:8888"
      https_proxy: "http://{{ hostvars[groups['proxy'][0]]['ansible_tun0']['ipv4']['address'] }}:8888"
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking if flannel deploy exists
    shell: "kubectl get deploy --namespace=kube-system  flannel"
    ignore_errors: True
    register: calico_deployment
    when: inventory_hostname == groups["masters"][0]

  - name: Creating flannel
    shell: "kubectl create -f /tmp/flannel.yml"
    when:
      - inventory_hostname == groups["masters"][0]
      - calico_deployment.rc != 0

  # at this point ensure that kubedns is running before continue
  - name: Be sure kubedns is running ok 
    shell: 'kubectl get pods -n=kube-system | grep kube-dns'
    register: result
    until: result.stdout.find("3/3") != -1
    retries: 18
    delay: 10
    when: inventory_hostname == groups["masters"][0]

  # the we create the other masters
  - name: Creating /etc/kubernetes to other masters
    file:
      path: /etc/kubernetes
      state: directory
    when:
      - inventory_hostname != groups["masters"][0]

  - name: Distribute /etc/kubernetes/pki to other masters
    synchronize:
      src: /etc/kubernetes/pki/
      dest: /etc/kubernetes/pki
    when:
      # distribute the pki files on masters that are not the first master
      - "'masters' in  group_names"
      - inventory_hostname != groups["masters"][0]
    delegate_to: "{{ groups.masters[0] }}"

  - name: Initializing other masters
    shell: "kubeadm init --config /tmp/kubeadm_config"
    when:
      # this is the master host
      - "'masters' in group_names"
      # this is not the first master
      - inventory_hostname != groups['masters'][0]
      # master hasn't be already initialized
      - inventory_hostname not in hostvars[groups['masters'][0]]['kubectl_output'].stdout

  # fixing kubectl
  - name: kubectl config
    copy:
      src: /etc/kubernetes/kubelet.conf
      dest: /root/.kube/config
      remote_src: True
    when:
      - "'masters' in group_names"
      - inventory_hostname != groups["masters"][0]

  - name: Getting token
    shell: "kubeadm token list | tail -f | awk '{print $1}'"
    register: token
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking if kube-proxy is Running
    shell: "ps -ef | grep [k]ube-proxy"
    register: kube_proxy_running
    ignore_errors: True
    when: inventory_hostname != groups["masters"]

  - name: Joining cluster on other nodes
    shell: "kubeadm join --token={{ hostvars[groups['masters'][0]]['kubeadm_token'].stdout }} {{ api_floating_ip }}:6443"
    register: kubedadm_output
    when:
     - "'masters' not in group_names"
     - "'/usr/local/bin/kube-proxy' not in kube_proxy_running.stdout"

  - name: Kubedadm output
    debug: var=kubeadm_outpout
    when:
     - "'masters' not in group_names"
     - "'/usr/local/bin/kube-proxy' not in kube_proxy_running.stdout"

  # remove this wait and had a test to check all nodes are ready
  - name: Wait for all nodes to be ready
    shell: "kubectl get nodes"
    register: result
    until: result.stdout.find("NotReady") == -1
    retries: 200
    delay: 10
    when:
     - inventory_hostname == groups['masters'][0]

  when:
    - ansible_distribution == "Ubuntu"
    - groups.masters|length > 1
